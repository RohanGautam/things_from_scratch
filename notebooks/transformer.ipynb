{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src: [Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.\n",
    "](https://www.youtube.com/watch?v=ISNdQcPhsts) by Umar Jamil.\n",
    "\n",
    "I've made some modifications as I worked though it, for example, using pytorch lightning to organise the training and validation logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention is all you need\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/transformer.png\" alt=\"transformer architecture\" width=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # the embedding dimension, called d_model in the attention is all you need paper.\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        # nn.Embedding maps indices (here, indices of words in the vocabulary) to the same\n",
    "        # tensor (a key-value lookup). The embeddings themselves are learnt as part of the model training.\n",
    "        # num_embeddings is same as vocab size, as an embedding is learnt for each item in the vocabulary\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, indices):\n",
    "        return self.emb(indices) * np.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "indices = torch.tensor([1, 123, 678, 21, 90])\n",
    "ie = InputEmbedding(512, 10_000)\n",
    "probs = ie(indices)\n",
    "print(indices.shape)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1305, 0.9885, 0.0000, 0.0000, 0.0000, 0.8463, 0.9005, 0.8272, 1.1029,\n",
       "        0.4151])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = nn.Dropout(0.3)\n",
    "# will zero ~3 elements at random. Used for regularisation during training\n",
    "t = torch.rand(10)\n",
    "d(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"this is only defined once and used during training and inference\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # maximum sequence length you expect to see, so that we can generate positional encodings\n",
    "        # upto that length\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # the positional encoding will be same dimension as the embedding for the sequence\n",
    "        self.pe = torch.zeros(seq_len, d_model)\n",
    "        self.set_positional_encoding()\n",
    "\n",
    "    def set_positional_encoding(self):\n",
    "        \"\"\"Positional encoding, as described in the paper: attention is all you need.\n",
    "        These can be predefined or learned, but the authors found no difference and chose this\n",
    "        as it would allow it to generalize over sequence lengths greater than ones seen during training.\n",
    "        \"\"\"\n",
    "        # pos from the paper\n",
    "        position = torch.arange(0, self.seq_len, 1, dtype=torch.float).unsqueeze(1)\n",
    "        # 2i from the paper: i is the dimension, here we operate on the even dimensions for both sin and cos\n",
    "        # we apply the sin'd and cos'd sequences to even and odd dimensions respectively\n",
    "        i_2 = torch.arange(0, self.d_model, 2)\n",
    "        # numerically stable way of computing 1/((10_000)^(2i/d_model)) from the paper\n",
    "        denominator = torch.exp((i_2 / self.d_model) * np.log(1e4))\n",
    "        # across all seq, just even,odd dimensions\n",
    "        self.pe[:, 0::2] = torch.sin(position / denominator)\n",
    "        self.pe[:, 1::2] = torch.cos(position / denominator)\n",
    "        # add batch dimension at start\n",
    "        # pe is (1,seq_len, d_model)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        # # a buffer is part of a module's state (state_dict) when saved, but is not a parameter that is\n",
    "        # # tuned.\n",
    "        # self.register_buffer(\"pe\", self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has a batch dimension\n",
    "        # x shape is (batch, seq_len, d_model)\n",
    "        # we add only the pe corresponding to x's sequence length\n",
    "        if self.pe.device != x.device:\n",
    "            self.pe = self.pe.to(x.device)\n",
    "        x += (self.pe[:, : x.shape[1], :]).requires_grad_(False)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "indices = torch.tensor([1, 123, 678, 21, 90]).unsqueeze(0)  # with batch dim\n",
    "ie = InputEmbedding(512, 10_000)\n",
    "max_seq_len = 1000\n",
    "pe = PositionalEncoding(512, max_seq_len, 0.3)\n",
    "\n",
    "\n",
    "probs = ie(indices)\n",
    "print(indices.shape)\n",
    "print(probs.shape)\n",
    "out_pe = pe(probs)\n",
    "print(out_pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In batchnorm, we calculate the mean, std dev across the batch dimension, ie, statistics\n",
    "# across the batch, one per feature. after normalisation, we do an affine transform\n",
    "# (m,b from y=mx+b), and m,b are learnable. this is so that it doesnt necessarily stick to zero mean\n",
    "# and unit variance (which would limit model expressivity).\n",
    "# But batchnorm will get representative mean,var only if batch size is a good enough size. Layernorm\n",
    "# is batch size independant, and the mean,var is calculated across all features, for each input example independantly,\n",
    "# and also applied to that input example alone independantly. It's used in transformers, rnn, etc\n",
    "# where batch sizes may vary. see https://arxiv.org/abs/1607.06450\n",
    "# note this has nothing to do with neural netowork layers, just the way it's applied to the data within the network\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, eps=1e-7) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.m = nn.Parameter(torch.tensor([1.0]))\n",
    "        self.c = nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stats along the feature dimension\n",
    "        # can also do keepdim=True instead of collapsing along mean dimension and adding it later\n",
    "        mu = torch.mean(x, dim=-1).unsqueeze(-1)\n",
    "        std = torch.std(x, dim=-1).unsqueeze(-1)\n",
    "        # again, the learnable params m and c allow not nonzero mean and non-unit variance\n",
    "        return self.m * ((x - mu) / (std + self.eps)) + self.c\n",
    "\n",
    "\n",
    "t = torch.rand(32, 100, 3)\n",
    "ln = LayerNorm()\n",
    "ln(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # self.d_model = d_model\n",
    "        # self.n_hidden = n_hidden\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# batch,n,dim\n",
    "# can give any number of examples (n)\n",
    "# features is what each neuron operates on and learns a function of them, for the output.\n",
    "t = torch.rand(32, 100, 512)\n",
    "ffn = FFN(512, 2048, 0.3)\n",
    "ffn(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # the embedding dimension will be split into n_heads equal chunks, with\n",
    "        # each chunk (possibly) attending to it differently\n",
    "        assert d_model % n_heads == 0\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(k, q, v, mask, dropout: nn.Dropout):\n",
    "        d_k = q.shape[-1]\n",
    "        # similarity (as a probability distribution) between what i have that is useful to other parts of me\n",
    "        # , and what i need. Scale by root of the multi head attention dimension\n",
    "        # shape is (batch, n_heads, seq_len, seq_len)\n",
    "        similarities: torch.Tensor = (k @ q.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        # mask_shape= (1, 1, seq_len, seq_len). make it a small value so softmax zeros it out\n",
    "        if mask:\n",
    "            # similarities[mask] = -torch.inf\n",
    "            # NOTE this is an additive mask. we just set the appropriate locations to -inf, same as adding zero everywhere else and -inf/small value in parts we want to hide\n",
    "            similarities.masked_fill_(mask == 0, -torch.inf)\n",
    "        similarities_probability = F.softmax(similarities, dim=-1)\n",
    "        if dropout:\n",
    "            similarities_probability = dropout(similarities_probability)\n",
    "        # gather appropriate values as a weighted(weights learnt) sum\n",
    "        attention = similarities_probability @ v\n",
    "        # similarities_probability used for visualisation\n",
    "        return attention, similarities_probability\n",
    "\n",
    "    def forward(self, k, q, v, mask):\n",
    "        # for encoder, k,q,v all are input (x), but seperated it here because in the decoder, some might be form\n",
    "        # the encoder and some might be from the decoder\n",
    "        # x : (batch, seq_len, dim)\n",
    "        # https://youtu.be/XfpMkf4rD6E?list=LL&t=1472\n",
    "        # there are the (for me's) below as this is self attention\n",
    "        k = self.Wk(k)  # what info do i have that is useful (for me)\n",
    "        q = self.Wq(q)  # what info do i need (from me)?\n",
    "        v = self.Wv(v)  # what info I publicly reveal (to other parts of me)?\n",
    "\n",
    "        # split k,q,v along embedding dimension into h parts\n",
    "        # we want (batch, head, seq, d_k) : each head should see the whole seq\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(v.shape[0], v.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        mask = None\n",
    "        # out has a shape batch, heads, seq_len, dk\n",
    "        attention, self.similarity_prob = MultiHeadAttention.attention(\n",
    "            k, q, v, mask, self.dropout\n",
    "        )\n",
    "        # view requires the tensor to be in continuous memory, which transpose disturbs, so we make it contiguous after transpose\n",
    "        attention = (\n",
    "            attention.transpose(1, 2).contiguous().view(k.shape[0], -1, self.d_model)\n",
    "        )\n",
    "        output = self.Wo(attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "t = torch.rand(32, 100, 512)\n",
    "mask = torch.ones(32, 1, 100)\n",
    "mha = MultiHeadAttention(512, 4, 0.3)\n",
    "# k,q,v all form the same input\n",
    "mha(t, t, t, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Linear(512, 512)\n",
    "len(list(l.parameters())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 100, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(32, 4, 100, 128)\n",
    "\n",
    "# q = torch.rand(512, 512)\n",
    "\n",
    "(F.softmax((t @ t.transpose(-1, -2)), dim=1) @ (torch.rand(32, 4, 100, 128))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormAndAdd(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, prev_layer):\n",
    "        # take the input to the previous layer, add it with the output of the previous layer (when fed in NORMALISED input)\n",
    "        # this is different from the add&norm in the original paper, and is one of the few improvements to the original architecture.\n",
    "        # the previous layer takes in normalised INPUT, not normalising the output. This prevents exploding gradients, etc and helps\n",
    "        # in better training dynamics\n",
    "        return self.dropout(prev_layer(self.norm(x))) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, mha: MultiHeadAttention, ffn: FFN, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = mha\n",
    "        self.ffn = ffn\n",
    "        # residual connections - two because they have seperate layernorms and layernorm parameters\n",
    "        self.rc1 = NormAndAdd(dropout)\n",
    "        self.rc2 = NormAndAdd(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        out = self.rc1(x, lambda x: self.mha(x, x, x, src_mask))\n",
    "        out = self.rc2(out, self.ffn)\n",
    "        return out\n",
    "\n",
    "\n",
    "t = torch.rand(32, 100, 512)\n",
    "mask = torch.ones(32, 100, 100)\n",
    "mha = MultiHeadAttention(512, 4, 0.3)\n",
    "ffn = FFN(512, 2048, 0.3)\n",
    "ecl = EncoderLayer(mha, ffn, 0.3)\n",
    "\n",
    "ecl(t, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = self.norm(layer(x, src_mask))\n",
    "        return x\n",
    "\n",
    "\n",
    "t = torch.rand(32, 100, 512)\n",
    "mask = torch.ones(32, 100, 100)\n",
    "\n",
    "\n",
    "encoder = Encoder(\n",
    "    nn.ModuleList(\n",
    "        [\n",
    "            EncoderLayer(MultiHeadAttention(512, 4, 0.3), FFN(512, 2048, 0.3), 0.3)\n",
    "            for _ in range(10)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "encoder(t, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [128, 1] but got: [128, 100].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 72\u001b[0m\n\u001b[1;32m     56\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     59\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(\n\u001b[1;32m     60\u001b[0m     nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     61\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[87], line 47\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, encoder_out, src_mask, trg, trg_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     41\u001b[0m     encoder_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     trg_mask,\n\u001b[1;32m     45\u001b[0m ):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 47\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[87], line 28\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, encoder_out, src_mask, trg, trg_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc1(trg, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmha(x, x, x, trg_mask))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# query is from the decoder, the encoder answers\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc3(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mNormAndAdd.forward\u001b[0;34m(self, x, prev_layer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, prev_layer):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# take the input to the previous layer, add it with the output of the previous layer (when fed in NORMALISED input)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# this is different from the add&norm in the original paper, and is one of the few improvements to the original architecture.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# the previous layer takes in normalised INPUT, not normalising the output. This prevents exploding gradients, etc and helps\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# in better training dynamics\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43mprev_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m x\n",
      "Cell \u001b[0;32mIn[87], line 28\u001b[0m, in \u001b[0;36mDecoderLayer.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc1(trg, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmha(x, x, x, trg_mask))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# query is from the decoder, the encoder answers\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc2(out, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc3(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, k, q, v, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# out has a shape batch, heads, seq_len, dk\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m attention, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_prob \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# view requires the tensor to be in continuous memory, which transpose disturbs, so we make it contiguous after transpose\u001b[39;00m\n\u001b[1;32m     56\u001b[0m attention \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     57\u001b[0m     attention\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m     58\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mMultiHeadAttention.attention\u001b[0;34m(k, q, v, mask, dropout)\u001b[0m\n\u001b[1;32m     29\u001b[0m     similarities_probability \u001b[38;5;241m=\u001b[39m dropout(similarities_probability)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# gather appropriate values as a weighted(weights learnt) sum\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43msimilarities_probability\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# similarities_probability used for visualisation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention, similarities_probability\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [128, 1] but got: [128, 100]."
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mmha: MultiHeadAttention,\n",
    "        mha: MultiHeadAttention,\n",
    "        ffn: FFN,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # masked multi head attention also in this. this is the self attention\n",
    "        self.mmha = mmha\n",
    "        # will be cross attention, as k,q come from encoder\n",
    "        self.mha = mha\n",
    "        self.ffn = ffn\n",
    "        self.rc1 = NormAndAdd(dropout)\n",
    "        self.rc2 = NormAndAdd(dropout)\n",
    "        self.rc3 = NormAndAdd(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_out,\n",
    "        src_mask,\n",
    "        trg,\n",
    "        trg_mask,\n",
    "    ):\n",
    "        out = self.rc1(trg, lambda x: self.mmha(x, x, x, trg_mask))\n",
    "        # query is from the decoder, the encoder answers\n",
    "        out = self.rc2(out, lambda x: self.mha(encoder_out, x, encoder_out, src_mask))\n",
    "        out = self.rc3(out, self.ffn)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_out,\n",
    "        src_mask,\n",
    "        trg,\n",
    "        trg_mask,\n",
    "    ):\n",
    "        for layer in self.layers:\n",
    "            x = self.norm(layer(encoder_out, src_mask, trg, trg_mask))\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_out = torch.rand(32, 100, 512)\n",
    "tout = torch.rand(32, 1, 512)\n",
    "src_mask = torch.ones(32,1, 1, 100)\n",
    "# src_mask = torch.ones(32,100, 100)\n",
    "# trg_mask = torch.ones(32, 100, 100)\n",
    "trg_mask = torch.ones(32, 1,100)\n",
    "\n",
    "\n",
    "decoder = Decoder(\n",
    "    nn.ModuleList(\n",
    "        [\n",
    "            DecoderLayer(\n",
    "                MultiHeadAttention(512, 4, 0.3),\n",
    "                MultiHeadAttention(512, 4, 0.3),\n",
    "                FFN(512, 2048, 0.3),\n",
    "                0.3,\n",
    "            )\n",
    "            for _ in range(10)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "decoder(encoder_out, src_mask, tout, trg_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # features will go from d_model to vocab_size\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch,seq_len,d_model) -> (batch, seq_len, vocab_size)\n",
    "        # we want a probability over all the positions in the vocabulary\n",
    "        # return F.softmax(self.proj(x), dim=-1)\n",
    "        # more numerically stable, can always exp later to get true non-log probabilities\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc: Encoder,\n",
    "        dec: Decoder,\n",
    "        src_emb: InputEmbedding,\n",
    "        trg_emb: InputEmbedding,\n",
    "        src_pos: PositionalEncoding,\n",
    "        trg_pos: PositionalEncoding,\n",
    "        proj: ProjectionLayer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        self.src_emb = src_emb\n",
    "        self.trg_emb = trg_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.trg_pos = trg_pos\n",
    "        self.proj = proj\n",
    "\n",
    "    def encode(self, src_indices, src_mask):\n",
    "        # indices: (batch, indices(padded))\n",
    "        x = self.src_emb(src_indices)\n",
    "        x = self.src_pos(x)\n",
    "        x = self.enc(x, src_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, enc_out, src_mask, trg_indices, trg_mask):\n",
    "        x = self.trg_emb(trg_indices)\n",
    "        x = self.trg_pos(x)\n",
    "        x = self.dec(enc_out, src_mask, x, trg_mask)\n",
    "        return x\n",
    "    def project(self,decoder_out):\n",
    "        return self.proj(decoder_out)\n",
    "\n",
    "    def forward(self, src_indices, src_mask, trg_indices, trg_mask):\n",
    "        enc_out = self.encode(src_indices, src_mask)\n",
    "        decoded = self.decode(enc_out, src_mask, trg_indices, trg_mask)\n",
    "        probs = self.project(decoded)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    src_vocab_size: int,\n",
    "    trg_vocab_size: int,\n",
    "    d_model: int,\n",
    "    src_seq_len: int,\n",
    "    trg_seq_len: int,\n",
    "    encdec_layers: int,\n",
    "    n_heads: int,\n",
    "    dropout: float = 0.1,\n",
    "    d_hh: int = 2048,\n",
    "):\n",
    "    # as an example, this is for a translation task.\n",
    "    src_ie = InputEmbedding(d_model, src_vocab_size)\n",
    "    trg_ie = InputEmbedding(d_model, trg_vocab_size)\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    trg_pos = PositionalEncoding(d_model, trg_seq_len, dropout)\n",
    "\n",
    "    encoder = Encoder(\n",
    "        nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    MultiHeadAttention(d_model, n_heads, dropout),\n",
    "                    FFN(d_model, d_hh, dropout),\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(encdec_layers)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    MultiHeadAttention(d_model, n_heads, dropout),\n",
    "                    MultiHeadAttention(d_model, n_heads, dropout),\n",
    "                    FFN(d_model, d_hh, dropout),\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(encdec_layers)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    proj = ProjectionLayer(d_model, trg_vocab_size)\n",
    "    transformer = Transformer(encoder, decoder, src_ie, trg_ie, src_pos, trg_pos, proj)\n",
    "\n",
    "    # xavier init\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # in place op if ends with an underscore.\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # \"tokenizer_path\": \"./data/\",\n",
    "    \"translation\": \"en-fr\",\n",
    "    \"tr_src\": \"en\",\n",
    "    \"tr_tgt\": \"fr\",\n",
    "    \"max_seq_len\": 500,\n",
    "    \"batch_size\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 127085\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"opus_books\", config[\"translation\"], split=\"train\")\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown token id is 0\n",
      "original: hello my name is senor bean\n",
      "encoded: [8, 12, 13, 4, 0, 6]\n",
      "decoded: hello my name is [UNK] bean\n"
     ]
    }
   ],
   "source": [
    "s = \"hello my name is senor bean\"\n",
    "t = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "t.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=1\n",
    ")\n",
    "# senor not in vocabulary\n",
    "t.train_from_iterator(\n",
    "    [\n",
    "        [\n",
    "            \"hello i like bean\",\n",
    "            \"ola\",\n",
    "            \"my neck hurts\",\n",
    "            \"is the earth round\",\n",
    "            \"what is your name?\",\n",
    "        ]\n",
    "    ],\n",
    "    trainer=trainer,\n",
    ")\n",
    "print(\"unknown token id is\", t.token_to_id(\"[UNK]\"))\n",
    "ids = t.encode(s).ids\n",
    "print(\"original:\", s)\n",
    "print(\"encoded:\", ids)\n",
    "print(\"decoded:\", t.decode(ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x378152a30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        # semantics based on how the dataset is organised\n",
    "        yield item[\"translation\"][lang]\n",
    "\n",
    "\n",
    "def get_tokenizer(ds, lang):\n",
    "    # i'll skip caching the tokenizer, it's not super expensive\n",
    "    t = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    t.pre_tokenizer = Whitespace()\n",
    "    trainer = WordLevelTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "    )\n",
    "    # senor not in vocabulary\n",
    "    t.train_from_iterator(get_sentences(ds, lang), trainer=trainer)\n",
    "    return t\n",
    "\n",
    "\n",
    "get_tokenizer(hf_dataset, config[\"tr_src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x32fb76710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def causal_mask(size):\n",
    "    # to ensure that in the encoder, the attention is applied only to the past values from the current token\n",
    "    m = torch.ones(1, size, size)\n",
    "    # only the lower triangle will be one\n",
    "    return torch.tril(m).bool()\n",
    "\n",
    "\n",
    "m = causal_mask(10).squeeze(0)\n",
    "plt.imshow(m)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLingualDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, hf_dataset, src_tokenizer, tgt_tokenizer, src_lang, tgt_lang, max_seq_len\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.pad_token_id = torch.tensor([src_tokenizer.token_to_id(\"[PAD]\")]).int()\n",
    "        self.sos_token_id = torch.tensor([src_tokenizer.token_to_id(\"[SOS]\")]).int()\n",
    "        self.eos_token_id = torch.tensor([src_tokenizer.token_to_id(\"[EOS]\")]).int()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        src_text = self.hf_dataset[idx][\"translation\"][self.src_lang]\n",
    "        tgt_text = self.hf_dataset[idx][\"translation\"][self.tgt_lang]\n",
    "\n",
    "        src_encoded = self.src_tokenizer.encode(src_text).ids\n",
    "        tgt_encoded = self.src_tokenizer.encode(tgt_text).ids\n",
    "\n",
    "        # src encoded (encoder input) will have [sos],[eos]\n",
    "        src_padding_len = self.max_seq_len - (len(src_encoded) + 2)\n",
    "        # target_encoded (the decoder input) will not have [eos], has [sos]\n",
    "        # the label(decoder output) will not have [sos], but will have [eos]\n",
    "        # this is because the label is the same as the decoder input shifted by one\n",
    "        # (since this is next token prediction)\n",
    "        tgt_padding_len = self.max_seq_len - (len(tgt_encoded) + 1)\n",
    "\n",
    "        if (src_padding_len < 0) or (tgt_padding_len < 0):\n",
    "            raise Exception(\"max_seq_len too small\")\n",
    "\n",
    "        encoder_inp = torch.cat(\n",
    "            [\n",
    "                self.sos_token_id,\n",
    "                torch.tensor(src_encoded),\n",
    "                self.eos_token_id,\n",
    "                torch.tensor([self.pad_token_id for _ in range(src_padding_len)]),\n",
    "            ]\n",
    "        ).int()\n",
    "        decoder_inp = torch.cat(\n",
    "            [\n",
    "                self.sos_token_id,\n",
    "                torch.tensor(tgt_encoded),\n",
    "                torch.tensor([self.pad_token_id for _ in range(tgt_padding_len)]),\n",
    "            ]\n",
    "        ).int()\n",
    "        labels = torch.cat(\n",
    "            [\n",
    "                torch.tensor(tgt_encoded),\n",
    "                self.eos_token_id,\n",
    "                torch.tensor([self.pad_token_id for _ in range(tgt_padding_len)]),\n",
    "            ]\n",
    "        ).int()\n",
    "        assert encoder_inp.shape[-1] == self.max_seq_len\n",
    "        assert decoder_inp.shape[-1] == self.max_seq_len\n",
    "        return {\n",
    "            # (seq,)\n",
    "            \"encoder_in\": encoder_inp,\n",
    "            # (seq,)\n",
    "            \"decoder_in\": decoder_inp,\n",
    "            # (seq,)\n",
    "            \"labels\": labels,\n",
    "            # add batch dim, second sequence dim , (1,1,seq)\n",
    "            \"encoder_mask\": (encoder_inp != self.pad_token_id)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int(),\n",
    "            # (1,seq,seq)\n",
    "            \"decoder_mask\": (decoder_inp != self.pad_token_id)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int()\n",
    "            & causal_mask(decoder_inp.shape[0]),\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = get_tokenizer(hf_dataset, config[\"tr_src\"])\n",
    "tgt_tokenizer = get_tokenizer(hf_dataset, config[\"tr_tgt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max src len: 471\n",
      "max tgt len: 482\n"
     ]
    }
   ],
   "source": [
    "max_src_len, max_tgt_len = 0, 0\n",
    "for item in hf_dataset:\n",
    "    # calculate maximum token lengths\n",
    "    max_src_len = max(\n",
    "        max_src_len,\n",
    "        len(src_tokenizer.encode(item[\"translation\"][config[\"tr_src\"]]).ids),\n",
    "    )\n",
    "    max_tgt_len = max(\n",
    "        max_tgt_len,\n",
    "        len(tgt_tokenizer.encode(item[\"translation\"][config[\"tr_tgt\"]]).ids),\n",
    "    )\n",
    "\n",
    "# this will inform config['max_seq_len']\n",
    "print(f\"max src len: {max_src_len}\")\n",
    "print(f\"max tgt len: {max_tgt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500])\n",
      "torch.Size([1, 1, 500])\n",
      "torch.Size([500])\n",
      "torch.Size([1, 500, 500])\n",
      "torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "# just to test things\n",
    "d = BiLingualDataset(\n",
    "    hf_dataset,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    config[\"tr_src\"],\n",
    "    config[\"tr_tgt\"],\n",
    "    config[\"max_seq_len\"],\n",
    ")\n",
    "\n",
    "probs = d[0]\n",
    "print(probs[\"encoder_in\"].shape)\n",
    "print(probs[\"encoder_mask\"].shape)\n",
    "print(probs[\"decoder_in\"].shape)\n",
    "print(probs[\"decoder_mask\"].shape)\n",
    "print(probs[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(hf_dataset))\n",
    "val_size = len(hf_dataset) - train_size\n",
    "\n",
    "train_hf_ds, val_hf_ds = random_split(hf_dataset, [train_size, val_size])\n",
    "train_ds = BiLingualDataset(\n",
    "    train_hf_ds,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    config[\"tr_src\"],\n",
    "    config[\"tr_tgt\"],\n",
    "    config[\"max_seq_len\"],\n",
    ")\n",
    "val_ds = BiLingualDataset(\n",
    "    val_hf_ds,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    config[\"tr_src\"],\n",
    "    config[\"tr_tgt\"],\n",
    "    config[\"max_seq_len\"],\n",
    ")\n",
    "train_loader = DataLoader(train_ds, config[\"batch_size\"], shuffle=True)\n",
    "# batch size one, as we dont want to average statistics across the batch for the validation step,\n",
    "# at least at the experimentation stage.\n",
    "val_loader = DataLoader(val_ds, 1, shuffle=False)\n",
    "\n",
    "model = build_transformer(\n",
    "    src_tokenizer.get_vocab_size(),\n",
    "    tgt_tokenizer.get_vocab_size(),\n",
    "    64,\n",
    "    config[\"max_seq_len\"],\n",
    "    config[\"max_seq_len\"],\n",
    "    3,\n",
    "    2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = next(iter(train_loader))\n",
    "example_input['encoder_in'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward pass, just for calculating example shapes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  | Name          | Type               | Params | Mode  | In sizes                                                           | Out sizes       \n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "0 | model         | Transformer        | 7.5 M  | train | [[16, 500], [16, 1, 1, 500], [16, 500], [16, 1, 500, 500]]         | [16, 500, 30000]\n",
       "1 | model.enc     | Encoder            | 842 K  | train | [[16, 500, 64], [16, 1, 1, 500]]                                   | [16, 500, 64]   \n",
       "2 | model.dec     | Decoder            | 892 K  | train | [[16, 500, 64], [16, 1, 1, 500], [16, 500, 64], [16, 1, 500, 500]] | [16, 500, 64]   \n",
       "3 | model.src_emb | InputEmbedding     | 1.9 M  | train | [16, 500]                                                          | [16, 500, 64]   \n",
       "4 | model.trg_emb | InputEmbedding     | 1.9 M  | train | [16, 500]                                                          | [16, 500, 64]   \n",
       "5 | model.src_pos | PositionalEncoding | 0      | train | [16, 500, 64]                                                      | [16, 500, 64]   \n",
       "6 | model.trg_pos | PositionalEncoding | 0      | train | [16, 500, 64]                                                      | [16, 500, 64]   \n",
       "7 | model.proj    | ProjectionLayer    | 1.9 M  | train | [16, 500, 64]                                                      | [16, 500, 30000]\n",
       "8 | loss_fn       | CrossEntropyLoss   | 0      | train | [[8000, 30000], [8000]]                                            | ?               \n",
       "9 | acc           | MulticlassAccuracy | 0      | train | ?                                                                  | ?               \n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "7.5 M     Trainable params\n",
       "0         Non-trainable params\n",
       "7.5 M     Total params\n",
       "30.101    Total estimated model params size (MB)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fastest way to define a device and parallelism agnostic training loop\n",
    "class LightningLoop(L.LightningModule):\n",
    "    def __init__(self, model: Transformer, example_input:dict):\n",
    "        super().__init__()\n",
    "        self.example_input_array = example_input\n",
    "        self.model = model\n",
    "        # contrary to how it sounds, ignore index actually takes in a _target_ VALUE to be ignored\n",
    "        # label smoothing makes the model less overconfident in any one output, and distributes the probs to others - to prevent overfitting\n",
    "        # take 0.1% of prob from highest prob and distribute among others\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=src_tokenizer.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    "        )\n",
    "        self.acc = torchmetrics.Accuracy(task='multiclass', num_classes=tgt_tokenizer.get_vocab_size())\n",
    "\n",
    "    def forward(self,**kwargs):\n",
    "        print('in forward pass, just for calculating example shapes')\n",
    "        # if example input array is a dict, passes as keywords to this function.\n",
    "        # This is just for passing the example input through the layers\n",
    "        batch = kwargs\n",
    "        encoder_inp = batch[\"encoder_in\"]\n",
    "        decoder_inp = batch[\"decoder_in\"]\n",
    "        labels = batch[\"labels\"]  # (batch,max_seq_len)\n",
    "        encoder_mask = batch[\"encoder_mask\"]\n",
    "        decoder_mask = batch[\"decoder_mask\"]\n",
    "        out = self.model(encoder_inp, encoder_mask, decoder_inp, decoder_mask)\n",
    "        out = out.view(-1, tgt_tokenizer.get_vocab_size())\n",
    "        loss = self.loss(out, labels.view(-1).long())\n",
    "        return loss\n",
    "\n",
    "    def loss(self, input, target):\n",
    "        return self.loss_fn(input, target)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        encoder_inp = batch[\"encoder_in\"]\n",
    "        decoder_inp = batch[\"decoder_in\"]\n",
    "        labels = batch[\"labels\"]  # (batch,max_seq_len)\n",
    "        encoder_mask = batch[\"encoder_mask\"]\n",
    "        decoder_mask = batch[\"decoder_mask\"]\n",
    "        # will have shape (batch, max_seq_len, tgt_vocab_size)\n",
    "        out = self.model(encoder_inp, encoder_mask, decoder_inp, decoder_mask)\n",
    "        # (batch, max_seq_len, tgt_vocab_size) -> (batch* max_seq_len, tgt_vocab_size)\n",
    "        out = out.view(-1, tgt_tokenizer.get_vocab_size())\n",
    "        # compare (batch* max_seq_len, tgt_vocab_size) with (batch* max_seq_len)\n",
    "        # the crossentropyloss function can take in the input like this.\n",
    "        loss = self.loss(out, labels.view(-1))\n",
    "        self.log(\"Loss/train\", loss, prog_bar=True)\n",
    "        _, pred_ids = torch.max(out,dim=1)\n",
    "        self.log(\"Accuracy/train\", self.acc(pred_ids, labels.view(-1)), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        encoder_inp = batch[\"encoder_in\"]\n",
    "        decoder_inp = batch[\"decoder_in\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        encoder_mask = batch[\"encoder_mask\"]\n",
    "        decoder_mask = batch[\"decoder_mask\"]\n",
    "        out = self.model(encoder_inp, encoder_mask, decoder_inp, decoder_mask)\n",
    "        out = out.view(-1, tgt_tokenizer.get_vocab_size())\n",
    "        loss = self.loss(out, labels.view(-1))\n",
    "        self.log(\"Loss/val \", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "# might have to move the model to cpu before running this\n",
    "lightning_loop = LightningLoop(model, example_input)\n",
    "ModelSummary(lightning_loop, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,ids = torch.max(torch.rand(500,3000),dim=1)\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit on a single batch as a test\n",
    "trainer = L.Trainer(max_epochs=1000, log_every_n_steps=10,limit_train_batches=1)\n",
    "# trainer.fit(lightning_loop, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "trainer.fit(lightning_loop, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model:Transformer, src_indices,src_mask, src_text,trg_text):\n",
    "    start_token_idx = src_tokenizer.token_to_id('[SOS]')\n",
    "    end_token_idx = src_tokenizer.token_to_id('[EOS]')\n",
    "    encoder_out = model.encode(src_indices,src_mask)\n",
    "    print(encoder_out.shape)\n",
    "    print(src_mask.shape)\n",
    "    decoder_inp = torch.empty(1,1).fill_(start_token_idx).type_as(src_indices)\n",
    "\n",
    "    while True:\n",
    "        if decoder_inp.shape[-1]>config['max_seq_len']:\n",
    "            break\n",
    "        print(model.trg_emb(decoder_inp).shape)\n",
    "        decoder_mask = causal_mask(decoder_inp.shape[-1])\n",
    "        print(decoder_mask.shape)\n",
    "        decode_out = model.decode(\n",
    "            encoder_out, src_mask, decoder_inp, decoder_mask\n",
    "        )\n",
    "        # probs for the last token\n",
    "        probs = model.proj(decode_out[-1,:])\n",
    "        _,next_word_idx = torch.max(probs, dim=1)\n",
    "        decoder_inp = torch.cat([decoder_inp, torch.empty(1,1).fill_(next_word_idx).type_as(src_indices)])\n",
    "        if next_word_idx==end_token_idx:\n",
    "            break\n",
    "        print('ok')\n",
    "    pred_sentence = tgt_tokenizer.decode(decoder_inp.flatten(), skip_special_tokens=False)\n",
    "    print('src:', src_text)\n",
    "    print('tgt:', trg_text)\n",
    "    print('pred:', pred_sentence)\n",
    "\n",
    "# TODO!!!!!\n",
    "# model = model.cpu()\n",
    "# example_input=next(iter(val_loader))\n",
    "# greedy_search(model, example_input['encoder_in'], example_input['encoder_mask'],example_input['src_text'],example_input['tgt_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model_pt \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer(\n\u001b[1;32m      2\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      3\u001b[0m     nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     norm_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel_pt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_in\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_in\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/coral-agent/lib/python3.11/site-packages/torch/nn/modules/transformer.py:215\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    218\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[1;32m    219\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    220\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    221\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    222\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# this expects the encoding to already be done\n",
    "model_pt = nn.Transformer(\n",
    "    d_model=64,\n",
    "    nhead=2,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    dropout=0.1,\n",
    "    batch_first=True,\n",
    "    norm_first=True\n",
    ")\n",
    "model_pt(\n",
    "    example_input['encoder_in'],\n",
    "    example_input['decoder_in'],\n",
    "    example_input['encoder_mask'],\n",
    "    example_input['decoder_mask'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
